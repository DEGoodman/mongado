"""Search and Q&A API routes."""

import logging
import time
from typing import Any

from fastapi import APIRouter
from rapidfuzz import fuzz

from models import SearchRequest, SearchResponse, SearchResult

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api", tags=["search"])


def _fuzzy_match_text(query: str, text: str, threshold: int = 80) -> float:
    """
    Check if query fuzzy matches the text and return relevance score.

    - Queries < 3 chars: exact substring match only (for terms like "SRE")
    - Queries >= 3 chars: fuzzy match with 80% threshold (handles typos)

    Args:
        query: Search query (lowercase)
        text: Text to search in (lowercase)
        threshold: Minimum similarity score (0-100)

    Returns:
        Relevance score (0.0 = no match, 1.0 = fuzzy match, 2.0 = exact match)
    """
    # Short queries: require exact match (prevents false positives)
    if len(query) < 3:
        return 2.0 if query in text else 0.0

    # Exact match (highest score)
    if query in text:
        return 2.0

    # Fuzzy match: check if query is similar to any word in text
    # Only match against words of similar length to avoid false positives
    words = text.split()
    best_score = 0.0
    for word in words:
        # Only fuzzy match words that are within 2 chars of query length
        if abs(len(word) - len(query)) <= 2:
            similarity = fuzz.ratio(query, word)
            if similarity >= threshold:
                # Map 80-100 similarity to 0.8-1.0 score
                best_score = max(best_score, similarity / 100.0)

    return best_score


def _get_all_resources(static_articles: list, user_resources_db: list, notes_service: Any) -> list[dict[str, Any]]:
    """Get all searchable resources (articles + notes).

    Notes are normalized to have 'note_id' field to distinguish from articles.
    """
    all_notes = notes_service.list_notes(is_admin=True)

    # Normalize note structure to match article structure for search
    normalized_notes = []
    for note in all_notes:
        normalized_note = {
            "note_id": note.get("id"),  # Keep string ID as note_id
            "title": note.get("title", "Untitled"),
            "content": note.get("content", ""),
            "tags": note.get("tags", []),
            "created_at": note.get("created_at"),
        }
        normalized_notes.append(normalized_note)

    return static_articles + user_resources_db + normalized_notes


def _normalize_search_result(doc: dict[str, Any], score: float = 1.0) -> SearchResult:
    """Normalize a resource document into a consistent SearchResult."""
    # Determine if this is an article or note
    is_note = "note_id" in doc
    resource_type = "note" if is_note else "article"
    resource_id = doc.get("note_id") if is_note else doc.get("id")

    return SearchResult(
        id=resource_id,
        type=resource_type,
        title=doc.get("title", "Untitled"),
        content=doc.get("content", ""),
        score=score
    )


def create_search_router(
    static_articles: list,
    user_resources_db: list,
    notes_service: Any,
    ollama_client: Any,
    neo4j_adapter: Any
) -> APIRouter:
    """Create search router with dependencies injected.

    This allows the router to access global state from main.py without
    circular imports while still keeping routes organized.
    """

    @router.post("/search", response_model=SearchResponse)
    def search_resources(request: SearchRequest) -> SearchResponse:
        """
        Search across all resources (articles + notes).

        **Default:** Fast text search with fuzzy matching (instant)
        - Queries < 3 chars: exact match only (e.g., "SRE")
        - Queries >= 3 chars: fuzzy match with 80% threshold (handles typos like "biling" â†’ "billing")

        **Semantic mode:** AI-powered semantic search via Ollama (slower, opt-in)
        - Set semantic=true to use AI embeddings for semantic search
        - Finds conceptually related content even without keyword matches
        - Takes 15-30+ seconds depending on corpus size

        The default text search is instant and works even when Ollama is unavailable
        or slow, making it ideal for the main search UI.
        """
        start_time = time.time()

        logger.info("Search request received: query=%s, semantic=%s, limit=%s", request.query, request.semantic, request.top_k)
        all_resources = _get_all_resources(static_articles, user_resources_db, notes_service)

        # Default: Fast text search with fuzzy matching
        if not request.semantic:
            logger.debug("Using fast text search with fuzzy matching")
            query_lower = request.query.lower()

            # Score each document based on title and content matches
            scored_docs = []
            for doc in all_resources:
                title_score = _fuzzy_match_text(query_lower, doc.get("title", "").lower())
                content_score = _fuzzy_match_text(query_lower, doc.get("content", "").lower())

                # Title matches are weighted higher (2x)
                total_score = (title_score * 2.0) + content_score

                if total_score > 0:
                    # Debug: Check for None IDs before adding
                    is_note = "note_id" in doc
                    resource_id = doc.get("note_id") if is_note else doc.get("id")
                    if resource_id is None:
                        logger.error(
                            "Found resource with None ID! Query=%s, Title=%s, Type=%s, Keys=%s",
                            query_lower,
                            doc.get("title", "Unknown"),
                            "note" if is_note else "article",
                            list(doc.keys())
                        )
                        continue  # Skip resources with None IDs
                    scored_docs.append((doc, total_score))

            # Sort by score (descending) and take top_k
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            results = [_normalize_search_result(doc, score=score) for doc, score in scored_docs[:request.top_k]]

            duration = time.time() - start_time
            logger.info("Text search complete: %d results in %.2fs", len(results), duration)
            return SearchResponse(results=results, count=len(results))

        # Semantic mode: Use Ollama for AI-powered search (opt-in)
        logger.info("Using semantic search via Ollama (corpus size: %d)", len(all_resources))

        # Check if Ollama is available
        if not ollama_client.is_available():
            logger.warning("Ollama not available, falling back to text search with fuzzy matching")
            query_lower = request.query.lower()

            # Score each document based on title and content matches (same logic as text search)
            scored_docs = []
            for doc in all_resources:
                title_score = _fuzzy_match_text(query_lower, doc.get("title", "").lower())
                content_score = _fuzzy_match_text(query_lower, doc.get("content", "").lower())

                # Title matches are weighted higher (2x)
                total_score = (title_score * 2.0) + content_score

                if total_score > 0:
                    # Skip resources with None IDs
                    is_note = "note_id" in doc
                    resource_id = doc.get("note_id") if is_note else doc.get("id")
                    if resource_id is None:
                        logger.warning(
                            "Skipping resource with None ID in fallback search: Title=%s",
                            doc.get("title", "Unknown")
                        )
                        continue
                    scored_docs.append((doc, total_score))

            # Sort by score (descending) and take top_k
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            results = [_normalize_search_result(doc, score=score) for doc, score in scored_docs[:request.top_k]]

            duration = time.time() - start_time
            logger.info("Fallback text search complete: %d results in %.2fs", len(results), duration)
            return SearchResponse(results=results, count=len(results))

        # Try to use fast semantic search with precomputed embeddings from Neo4j
        if neo4j_adapter.is_available():
            logger.info("Using fast semantic search with precomputed embeddings from Neo4j")

            # Fetch precomputed embeddings for articles and notes
            embeddings_data = neo4j_adapter.get_all_embeddings()
            logger.info("Fetched %d precomputed embeddings from Neo4j", len(embeddings_data))

            if embeddings_data:
                # Build documents with embeddings
                # Need to merge embedding data with full document content
                documents_with_embeddings = []

                for emb_data in embeddings_data:
                    doc_id = emb_data["id"]
                    doc_type = emb_data["type"]

                    # Find the full document from all_resources
                    full_doc = next(
                        (doc for doc in all_resources
                         if (doc_type == "Article" and str(doc.get("id")) == doc_id) or
                            (doc_type == "Note" and doc.get("note_id") == doc_id)),
                        None
                    )

                    if full_doc:
                        # Combine full document with its embedding
                        doc_with_embedding = {**full_doc, "embedding": emb_data["embedding"]}
                        documents_with_embeddings.append(doc_with_embedding)

                logger.info("Matched %d documents with embeddings", len(documents_with_embeddings))

                # Perform fast semantic search
                semantic_results = ollama_client.semantic_search_with_precomputed_embeddings(
                    request.query, documents_with_embeddings, request.top_k
                )

                results = [
                    _normalize_search_result(doc, score=doc.get("score", 0.0))
                    for doc in semantic_results
                ]
                duration = time.time() - start_time
                logger.info("Fast semantic search complete: %d results in %.2fs", len(results), duration)
                return SearchResponse(results=results, count=len(results))
            else:
                logger.warning("No precomputed embeddings found, falling back to on-demand generation")

        # Fallback: Generate embeddings on-demand (slow but works without Neo4j)
        logger.info("Using on-demand embedding generation (slower)")
        semantic_results = ollama_client.semantic_search(request.query, all_resources, request.top_k)
        results = [
            _normalize_search_result(doc, score=doc.get("score", 0.0))
            for doc in semantic_results
        ]
        duration = time.time() - start_time
        logger.info("Semantic search complete: %d results in %.2fs", len(results), duration)
        return SearchResponse(results=results, count=len(results))



    return router
